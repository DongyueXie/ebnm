---
title: "Optimization tests"
author: "Matthew Stephens"
date: "December 17, 2017"
output: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulate data

Goal here is to do some tests of the optimization algorithms, speed
and accuracy.  First we simulate some data:

```{r}
set.seed(1)
n= 10000
s = 1
xnull = rnorm(n,0,s)
xnn = xnull
xnn[1] = 5*s[1] # a single 5 sigma observation
xalt = xnull + rnorm(n,0,1)
```

# Normal version

## Using gradients to optimize

First a quick illustration that using gradients can produce better results (smaller
value of objective).

```{r}
ebnm:::mle_normal(xnn,s)
ebnm:::mle_normal_logscale(xnn,s)
ebnm:::mle_normal_logscale_grad(xnn,s)
```

## Speed

For nearly null:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_normal(xnn,s),ebnm:::mle_normal_logscale_grad(xnn,s),times = 20)
```

For null:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_normal(xnull,s),ebnm:::mle_normal_logscale_grad(xnull,s),times = 20)
```

For alternative:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_normal(xalt,s),ebnm:::mle_normal_logscale_grad(xalt,s),times = 20)
```

Summary: the grad version is a bit slower for the "null" case - maybe there
is room for some improvement here.

## Stability

Also this example shows how the results on log scale are more stable.
This particular example is a bit weird and I stumbled on it by accident.
The null data contain one observation (575) with s very small and x approximately
3 times s. This causes the log-likelihood to be optimized at
a slightly weird value. (It is possible the loglikelihood here is multimodal?)
```{r}
set.seed(1)
n= 10000
s = rgamma(n,1,1) 
xnull = rnorm(n,0,s)
xnn = xnull
xnn[1] = 5*s[1] # a single 5 sigma observation
ebnm:::mle_normal(xnn,s)
ebnm:::mle_normal(1e6*xnn,1e6*s)
ebnm:::mle_normal_logscale(xnn,s)
ebnm:::mle_normal_logscale(1e6*xnn,1e6*s)
ebnm:::mle_normal_logscale_grad(xnn,s)
ebnm:::mle_normal_logscale_grad(1e6*xnn,1e6*s)
```


# Laplace version

## Gradients improve optimization

```{r}
ebnm:::mle_laplace(xnn,s)
ebnm:::mle_normal_logscale(xnn,s)
ebnm:::mle_normal_logscale_grad(xnn,s)
```

## Speed

Note that the optimization is much slower for Laplace. (The posterior
mean computations are also slower, not shown here...)

For nearly null:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_laplace(xnn,s),ebnm:::mle_laplace_logscale_grad(xnn,s),times = 20)
```

For null:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_laplace(xnull,s),ebnm:::mle_laplace_logscale_grad(xnull,s),times = 20)
```

For alternative:
```{r}
microbenchmark::microbenchmark(ebnm:::mle_laplace(xalt,s),ebnm:::mle_laplace_logscale_grad(xalt,s),times = 20)
```

## Stability

```{r}
set.seed(1)
n= 10000
s = rgamma(n,1,1) 
xnull = rnorm(n,0,s)
xnn = xnull
xnn[1] = 5*s[1] # a single 5 sigma observation
ebnm:::mle_laplace(xnn,s)
ebnm:::mle_laplace(1e6*xnn,1e6*s)
ebnm:::mle_laplace_logscale(xnn,s)
ebnm:::mle_laplace_logscale(1e6*xnn,1e6*s)
ebnm:::mle_laplace_logscale_grad(xnn,s)
ebnm:::mle_laplace_logscale_grad(1e6*xnn,1e6*s)
```

## Session info

This code chunk gives information about the computing environment,
including the version of R and the packages, used to generate the
results above.

```{r}
sessionInfo()
```
